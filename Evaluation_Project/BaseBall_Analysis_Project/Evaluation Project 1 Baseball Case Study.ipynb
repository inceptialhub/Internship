{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Baseball Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Statement:\n",
    "This dataset utilizes data from 2014 Major League Baseball seasons in order to develop an algorithm that predicts the number of wins for a given team in the 2015 season based on several different indicators of success. There are 16 different features that will be used as the inputs to the machine learning and the output will be a value that represents the number of wins.\n",
    "\n",
    "Input features: \n",
    "Runs, \n",
    "At Bats, \n",
    "Hits, \n",
    "Doubles, \n",
    "Triples, \n",
    "Homeruns, \n",
    "Walks, \n",
    "Strikeouts, \n",
    "Stolen Bases, \n",
    "Runs Allowed, \n",
    "Earned Runs, \n",
    "Earned Run Average (ERA), \n",
    "Shutouts, \n",
    "Saves, \n",
    "Complete Games and Errors\n",
    "\n",
    "Output: \n",
    "Number of predicted wins (W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For downloading the dataset, use the link given below.     \n",
    "Downlaod Files:\n",
    "<!-- https://github.com/dsrscientist/Data-Science-ML-Capstone-Projects/blob/master/baseball.csv -->\n",
    "https://github.com/dsrscientist/Data-Science-ML-Capstone-Projects/blob/master/baseball.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Importing require library for performing EDA, Data Wrangling and data cleaning</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for data wrangling purpose\n",
    "import numpy as np # Basic computation library\n",
    "import seaborn as sns # For Visualization \n",
    "import matplotlib.pyplot as plt # ploting package\n",
    "%matplotlib inline\n",
    "import warnings # Filtering warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importing case study CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('https://raw.githubusercontent.com/dsrscientist/Data-Science-ML-Capstone-Projects/master/baseball.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No of Rows:',df.shape[0])\n",
    "print('No of Columns:',df.shape[1])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ####  Comment -\n",
    "    - This dataset contains 16 features which contains statistics summary of the Baseball players and the 'W' (wins) is the target variable which predicts the number of wins.\n",
    "    - Input features in this dataset are : Runs, At Bats, Hits, Doubles, Triples, Homeruns, Walks, Strikeouts, Stolen Bases, Runs Allowed, Earned Runs, Earned Run Average (ERA), Shutouts, Saves, Complete Games and Errors\n",
    "    - Target features : Number of predicted wins (W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><em>The column names present in our data that is not making much sense and will need deciphering to be converted into understandable format. In order to gain understanding of different columns in dataset, following baseball stastics terminology I get from Wikipedia.</em><b>\n",
    "  \n",
    " \n",
    "- #### Pitching statistics:\n",
    "    - W – Win: number of games where pitcher was pitching while their team took the lead and went on to win, also the starter needs to pitch at least 5 innings of work\n",
    "    - RA – Run average: number of runs allowed times nine divided by innings pitched\n",
    "    - ER – Earned run: number of runs that did not occur as a result of errors or passed balls\n",
    "    - ERA – Earned run average: total number of earned runs (see \"ER\" above), multiplied by 9, divided by innings pitched\n",
    "    - CG – Complete game: number of games where player was the only pitcher for their team\n",
    "    - SHO – Shutout: number of complete games pitched with no runs allowed. A starting pitcher is credited with a shutout when he pitches the entire game for a team and does not allow the opposition to score. By definition, any pitcher who throws a shutout is also awarded a win. \n",
    "    - SV – Save: number of games where the pitcher enters a game led by the pitcher's team, finishes the game without surrendering the lead, is not the winning pitcher, and either (a) the lead was three runs or fewer when the pitcher entered the game; (b) the potential tying run was on base, at bat, or on deck; or (c) the pitcher pitched three or more innings\n",
    "\n",
    "    \n",
    "- #### Batting statistics:\n",
    "\n",
    "    - R – Runs scored: number of times a player crosses home plate\n",
    "    - AB – At bat: plate appearances, not including bases on balls, being hit by pitch, sacrifices, interference, or obstruction.  The number of times in which the hitter appeared at the plate and made a base hit, reached base on an error, or was out. \n",
    "    - H – Hit: reaching base because of a batted, fair ball without error by the defense\n",
    "    - 2B – Double: hits on which the batter reaches second base safely without the contribution of a fielding error\n",
    "    - 3B – Triple: hits on which the batter reaches third base safely without the contribution of a fielding error\n",
    "    - HR – Home runs: hits on which the batter successfully touched all four bases, without the contribution of a fielding error\n",
    "    - BB – Base on balls (also called a \"walk\"): hitter not swinging at four pitches called out of the strike zone and awarded first base.A walk (or base on balls) occurs when a pitcher throws four pitches out of the strike zone, none of which are swung at by the hitter. After refraining from swinging at four pitches out of the zone, the batter is awarded first base. \n",
    "    - K – Strike out (also abbreviated SO): number of times that a third strike is taken or swung at and missed, or bunted foul. Catcher must catch the third strike or batter may attempt to run to first base. It usually means the batter is out.\n",
    "\n",
    "    \n",
    "- #### Base running statistics:\n",
    "\n",
    "    - SB – Stolen base: number of bases advanced by the runner while the ball is in the possession of the defense.A stolen base occurs when a baserunner advances by taking a base to which he isn't entitled. This generally occurs when a pitcher is throwing a pitch, but it can also occur while the pitcher still has the ball or is attempting a pickoff, or as the catcher is throwing the ball back to the pitcher.\n",
    "    - R – Runs scored: times reached home plate legally and safely\n",
    "\n",
    "- #### Fielding statistics:\n",
    "\n",
    "    - E – Errors: number of times a fielder fails to make a play he should have made with common effort, and the offense benefits as a result. An error is an act, in the judgment of the official scorer, of a fielder misplaying a ball in a manner that allows a batter or baserunner to advance one or more bases or allows a plate appearance to continue after the batter should have been put out. \n",
    "\n",
    "<b><em>Now that we have clearer understanding on what the abbreviation mean and In order to simplify we are going to rename columns in dataset.</em></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'W' : 'Wins', \n",
    "                   'R' : 'Runs Scored', \n",
    "                  'AB' : 'At Bat', \n",
    "                   'H' : 'Hits', \n",
    "                  '2B' : 'Doubles', \n",
    "                  '3B' : 'Triples',\n",
    "                  'HR' : 'Home Runs', \n",
    "                  'BB' : 'Base on Balls', \n",
    "                  'SO' : 'Strike Outs', \n",
    "                  'SB' : 'Stolen Base',\n",
    "                  'RA' : 'Runs Average', \n",
    "                  'ER' : 'Earned Runs', \n",
    "                 'ERA' : 'Earned Run Average', \n",
    "                  'CG' : 'Complete Game',\n",
    "                 'SHO' : 'Shut Outs', \n",
    "                  'SV' : 'Saves', \n",
    "                   'E' : 'Errors'}, \n",
    "          inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ####  Comment -\n",
    "    - We can obsereve that this datset has only numeric data and no column has categorical data. \n",
    "    - This dataset fall into regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective :\n",
    "<b>Develop an ML Regression based algorithm that predicts the number of wins for a given team based on features. Here Wins is target variable and others are Input features.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the statistics of the columns using heatmap.\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.heatmap(df.describe(),linewidths = 0.1,fmt='0.1f',annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Comment -\n",
    "     - If we just look at mean and 50% columns for different feature we can see data is sightly right skew for most of features.\n",
    "     - Count is same for each variable.\n",
    "     - 75% and max values for Errors, Shutout, Run Scored shows presence of possible outliers.\n",
    "     - Overall all statstical parameter from mean to max, indicate <b> features are seem to be progressing in a definite manner showing no visible abnormalities.</b>\n",
    "     -  Heatmap clearly shows data need to scale while building ML Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing value check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,7))\n",
    "sns.heatmap(df.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum().sort_values(ascending = False)\n",
    "percentage_missing_values =(missing_values/len(df))*100\n",
    "print(pd.concat([missing_values, percentage_missing_values], axis =1, keys =['Missing Values', '% Missing data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Comment -\n",
    "    There is no null value present in dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we can try to bring insight in what feature contribute to win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,25), facecolor='white')\n",
    "plotnumber =1\n",
    "for column in df:\n",
    "    if plotnumber <=17:\n",
    "        ax = plt.subplot(6,3,plotnumber)\n",
    "        sns.distplot(df[column], color='r',hist=False,kde_kws={\"shade\": True})\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Comment -\n",
    "    - Clearly some of feature are either left or right skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette('gist_rainbow_r')\n",
    "plt.figure(figsize=(20,30), facecolor='white')\n",
    "plotnumber =1\n",
    "for column in df:\n",
    "    if plotnumber <=17:\n",
    "        ax = plt.subplot(6,3,plotnumber)\n",
    "        sns.violinplot(df[column])\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Comment -\n",
    "    - Shut outs and Complete Game occur very rarely now-a-days which we can definately see in violinplot of these variable.\n",
    "    - An Errors does not count as a hit but still counts as an at bat for the batter, So need to dive into how much Error are contributing to at bat.\n",
    "    - Most of saves are between 30 & 50. Saves doesnot entitle pitcher as wining pitcher but still it bring wins for team.It will be interesting what relation wins and save held or how much saves contribute in win.\n",
    "    - Run average, Earned run and Earned run average are important for pitcher statstics. We can see there is not much difference in plot of Earned run and Run Average, so from here we can conclude that Unearned Run doesnot making much difference in wins.\n",
    "    - Homeruns (125 to 175 peak) are more than triples (20 to 40 majority) so most of good shot by battar directly convert into homeruns.\n",
    "    - As we know pitcher try to keep Earned run Average low which eventually lead to wins. Here for most of game ERA is around 3.5-4.5.\n",
    "    - Let consider violinplot of doubles and base on balls. We know that if pitcher pitched ball for consecutive 4 ball then Base is awarded to batter. Clearly More runs comes from base of ball than doubles.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><em> Lets now Investigate Runs and Hits together , before that let dive into how team get win and some clearity over Run Vs Hits </b></em>\n",
    "- <b> How do u win baseball ? </b>\n",
    "    - To win in baseball, you must reach the end of the game with more runs than your opponent. If you have the same amount of runs, you will go into another inning until one team has more runs at the end of the inning.\n",
    "    \n",
    "- <b> Runs Vs Hits </b>\n",
    "    - Runs (R) - The number of runs scored by that player, by reaching home base in any manner. \n",
    "    - Hits (H) - The number of base hits made by that player, not including reaching base on an error or on a \"fielder's choice\".\n",
    "    \n",
    "<b><em> So Now we will put insight on how many hits convert into Runs and inturn lead to win throgh multivariate analysis </b></em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the relation between two variables\n",
    "sns.set_palette('Set1')\n",
    "plt.figure(figsize=[10,6])\n",
    "plt.title('Comparison between Runs and Hits',fontsize =20)\n",
    "sns.scatterplot(df['Runs Scored'],df['Hits'],hue=df['Wins'])\n",
    "plt.xlabel('Runs',fontsize =16)\n",
    "plt.ylabel(\"Hits\",fontsize =16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - #### Comment : \n",
    "      - Even if number of times ball hit bat is less than 1375 still run in range of 650 to 750 contribute to win. \n",
    "      - Very less wining chance for run less than 650 and no of hits less than 1325.\n",
    "      - There is one outlier in runs. After checking sknewness we can decide whether to keep to while building ML model or remove it even if it is valid data point. Definitely it will affect performance of ML model if we consider outlier data points as most of data point will not fall dont in that side. potential leading to biased model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the relation between two variables\n",
    "sns.set_palette('Set1')\n",
    "plt.figure(figsize=[10,6])\n",
    "plt.title('Runs Scored Vs Home Runs',fontsize =20)\n",
    "sns.scatterplot(df['Runs Scored'],df['Home Runs'],hue=df['Wins'])\n",
    "plt.xlabel('Runs Scored',fontsize =16)\n",
    "plt.ylabel('Home Runs',fontsize =16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Comment :\n",
    "    - Home Runs in range of <u> 140 & 180 </u> with combination of Run Scored in between <u> 650-750 </u> lead to more than 90 Wins for team. So keeping home runs in this range is cruical for more possibility of wins. \n",
    "    - But still its game, anything is possible with good performance of single winning player. We can see from scatter plot some of the datapoints have homeruns above 200, but still in wins are around 80.\n",
    "    \n",
    " <b> In conclusion we can say that Home runs is definitely contributing factor for team to win but not sufficient to make sure win. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the relation between two variables\n",
    "sns.set_palette('Set1')\n",
    "plt.figure(figsize=[10,6])\n",
    "plt.title('Comparison between Runs and At Bat', fontsize =20)\n",
    "sns.scatterplot(df['Runs Scored'],df['At Bat'],hue=df['Wins'])\n",
    "plt.xlabel('Runs Scored',fontsize =16)\n",
    "plt.ylabel(\"At Bat\",fontsize =16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Comment :\n",
    "    - We doesnot get any benchmark range for at bats from here. So <b>it is questionable things that how much At bats matter to winning statstics.</b> Atleast we get here that At Bat and Run Scored has positive linear relationship, which means that more Run Scroed naturally lead to more at bats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the relation between two variables\n",
    "sns.set_palette('Set1')\n",
    "plt.figure(figsize=[10,6])\n",
    "plt.title('Runs Scored Vs Strike Outs',fontsize =20)\n",
    "sns.scatterplot(df['Runs Scored'],df['Strike Outs'],hue=df['Wins'])\n",
    "plt.xlabel('Runs Scored',fontsize =16)\n",
    "plt.ylabel('Strike Outs',fontsize =16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Comment :\n",
    "    - In simple word strike Outs means batter is out. We can see Strike out opponent team below 700 runs essential for more win. \n",
    "    - Clearly Strikeout below 1200 is like <b><em>making recipe for losing game.</em></b> Strikeouts in regular interval not only lead to pressure on opponent in game but also bring break on high run score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the relation between two variables\n",
    "sns.set_palette('hsv')\n",
    "plt.figure(figsize=[10,6])\n",
    "plt.title('Errors Vs Earned Run Average',fontsize =20)\n",
    "sns.scatterplot(df['Errors'],df['Earned Run Average'],hue=df['Wins'], cmap=('Spectral'))\n",
    "plt.xlabel('Errors',fontsize =16)\n",
    "plt.ylabel('Earned Run Average',fontsize =16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Comment -\n",
    "    - <b><em>Same result about ERA we got here as we get in Violin Plot</em></b>.\n",
    "    - Keeping ERA below 3.5 or as much as low by Pitcher means sure win. By keeping low ERA pitcher saves run for his team.\n",
    "    - Another things we can see in above scatter plot is that <b> Minimum error means maximum wins.</b> Keeping Errors below number 90 can be held as benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the relation between two variables\n",
    "sns.set_palette('hsv')\n",
    "plt.figure(figsize=[10,6])\n",
    "plt.title('At Bat Vs Base on Balls',fontsize =20)\n",
    "sns.scatterplot(df['At Bat'],df['Earned Run Average'],hue=df['Errors'], cmap=('Spectral'))\n",
    "plt.xlabel('At Bat',fontsize =16)\n",
    "plt.ylabel('Earned Run Average',fontsize =16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Comment :\n",
    "    - At Bat Vs Base on Balls doesnot give any significant imformation than High ERA means High Errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,15), facecolor='white')\n",
    "plotnumber =1\n",
    "for column in df:\n",
    "    if plotnumber <=18:\n",
    "        ax = plt.subplot(6,3,plotnumber)\n",
    "        sns.boxplot(df[column], palette='hsv')\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Comment :\n",
    "    - There are some outliers present in data. But as data is of Real world MLB 2014, these outliers are valid datapoints.\n",
    "    - Shutouts are rare but completely possible with exceptional performance.\n",
    "    - Run scored has one outliers but as Run is one of the most important parameter, it will be wise to remove corresponding datpoint.\n",
    "    - Outliers in Errors are totally possible so now need to worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,12))\n",
    "sns.barplot(x=\"Wins\", y=\"Base on Balls\", data=df,palette='PiYG')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Comment :\n",
    "    - Base on ball is contribution from pitcher to batter for winning. In simple word it is like Wide or No Ball in Cricket.\n",
    "    - We can see that base on ball above 400 really contribute in Wins in team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,12))\n",
    "sns.barplot(x=\"Wins\", y=\"Runs Scored\", data=df,palette='PiYG')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Comment:\n",
    "    - Run Scored above 600 is benchmark for wining in any scenerio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,12))\n",
    "sns.barplot(x=\"Wins\", y=\"Runs Average\", data=df,palette='gist_earth')\n",
    "plt.title('Bar plot of Wins Vs Run Average', fontsize =20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Comment :\n",
    "    - Here comes interesting plot, we can see Run Average decrease with increase in number of wins. <b>But why this trend if more runs means directly increase in chance of winning.</b>\n",
    "    - <u> More Run Average gives advantage to hitter and here as wins are high even at low run this clearly means that perforamance of pitcher is better than hitter i.e. pitcher is knocking out hitter at low run score. This must be reflected in any pitcher side stastical parameter like Strike outs, Earned Run or Earned Run Average.</u>\n",
    "    \n",
    "<b><em> Lets try find answer of this millon dollor puzzle by diving more into relation of Wins with parameters like ERA, strikeouts.</b></em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,12))\n",
    "sns.barplot(x=\"Wins\", y=\"Earned Run Average\", data=df,palette='spring')\n",
    "plt.xlabel('Wins',fontsize =16)\n",
    "plt.ylabel('Earned Run Average',fontsize =16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Comment :\n",
    "    <b><em>And here we got what we looking for.As we thought there must be atleast one pitcher parameter where this decreasing trend must be reflected.</em>\n",
    "    - ERA is low for maximum wins. Low ERA by pitcher clearly means that pitcher is giving less chance given to batter to score run.\n",
    "    - We already come to conclusion of keeping ERA less than 3.5 in previous result. This plot also support that story.\n",
    "    - But this also indicate that <b> ERA and Runs Average score are bound to related with each other in linear relationship</b> (may be positive or negtive).\n",
    "    - Another things here is <b> that Earned Run and Wins must have negative relationship.</b> we will check it in further investigation.\n",
    "    \n",
    "<b> Let check it through strip plot between Runs Average and Earned Run Average </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the relation between two variables\n",
    "sns.set_palette('Set1')\n",
    "plt.figure(figsize=[16,8])\n",
    "plt.title('Comparison between Run Average and Earned Run Average', fontsize =20)\n",
    "sns.stripplot(df['Runs Average'],df['Earned Run Average'],hue=df['Wins'])\n",
    "plt.xlabel('Runs Average',fontsize =16)\n",
    "plt.ylabel(\"Earned Run Average\",fontsize =16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Comment :\n",
    "    Here we got what we suspect in previous plot. ERA and RA hold linear relationship.\n",
    "    \n",
    "<b> Now let check strikeout show same behavior as ERA with respect to Runs Average.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the relation between two variables\n",
    "sns.set_palette('Set1')\n",
    "plt.figure(figsize=[16,8])\n",
    "plt.title('Comparison between Run Average and Earned Run Average', fontsize =20)\n",
    "sns.stripplot(df['Runs Average'],df['Strike Outs'],hue=df['Wins'])\n",
    "plt.xlabel('Runs Average',fontsize =16)\n",
    "plt.ylabel(\"Strike Outs\",fontsize =16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Comment :\n",
    "    Strike outs are randomly placed giving not much any significant insights. Most probably strikeouts doesnot matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b><em> There is one outlier in Runs score, lets check that entry </b></em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Runs Scored'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Runs Scored']==891]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It seem like highest Doubles, Homeruns and base ball also belong to this entry. Let cross check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Home Runs'].max(),df['Base on Balls'].max(),df['Doubles'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we got what we suspect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><em> In one of the previous result we suspect that Earned Run/Earned Run Average and Wins must have negative relationship, let check by looking at jointplot.</b></em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"Earned Runs\", y=\"Wins\", data=df, color=\"blue\",palette=\"Set1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"Earned Run Average\", y=\"Wins\", data=df, color=\"green\",palette=\"Set1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><em> Jointplot shows same story about Earned Run/Earned Run Average and Wins having linear negative relationship</b></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let check relationship between saves and wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.jointplot(x=\"Saves\", y=\"Wins\", data=df, color=\"purple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Comment :\n",
    "    - A save is rewarded to the relief pitcher who finishes a game for the winning team under certain circumstances \n",
    "    - Here with increase in the number of save increases the number of wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue=\"Wins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,12))\n",
    "sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True, square=True, fmt='0.3f', \n",
    "            annot_kws={'size':10}, cmap=\"gist_stern\",)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers Detection and Removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "z = np.abs(zscore(df))\n",
    "threshold = 3\n",
    "df1 = df[(z<3).all(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+'Shape of dataset after removing outliers :'+\"\\033[0m\",df1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+'Percentage Data Loss :'+\"\\033[0m\",((30-29)/30)*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection and Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Skewness of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment -\n",
    "1. Optimal range for skewness is -0.5 to 0.5.\n",
    "2. Hits, Complete Game, Shuts Outs, Saves, Errors are positively Skewed in nature, need to transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming positive or right skew data using boxcox transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Hits']=boxcox(df1['Hits'],-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Shut Outs']=boxcox(df1['Shut Outs'],0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Saves']=boxcox(df1['Saves'],0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Other feature not able transform by Boxcox Method as they showing data must be positive. So others columns are transfrom using yeo-johnson method</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EC=['Errors','Complete Game']\n",
    "ds =df1[EC].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trans =ColumnTransformer(\n",
    "    [  ('Errors',PowerTransformer(method='yeo-johnson',standardize=True),['Errors']),\n",
    "      ('Complete Game',PowerTransformer(method='yeo-johnson',standardize=True),['Complete Game'])])\n",
    "transformed_yeojohnson =column_trans.fit_transform(df1)   \n",
    "new_cols=['Errors','Complete Game']\n",
    "dataset=pd.DataFrame(transformed_yeojohnson,columns=new_cols) #to convert numpy array back into dataframe\n",
    "pd.concat([dataset],axis=1)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reseting index and mergeing transform data\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "dataset.index=df1.index\n",
    "df1[EC]=dataset[EC]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skewness after transforming features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> All features skewness is now transform within permissible limit of -0.5 to 0.5 as shown above </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Corrleation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_triangle = np.triu(df.corr())\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.heatmap(df1.corr(), vmin=-1, vmax=1, annot=True, square=True, fmt='0.3f', \n",
    "            annot_kws={'size':10}, cmap=\"gist_stern\", mask=upper_triangle)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,6))\n",
    "df1.corr()['Wins'].drop(['Wins']).plot(kind='bar',color = 'c')\n",
    "plt.xlabel('Features',fontsize=15)\n",
    "plt.ylabel('Wins',fontsize=15)\n",
    "plt.title('Correlation of features with Target Variable win',fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Comment :\n",
    "    - Hits, At bats, Complete game and errors are very poorly correlated with target variable.\n",
    "    - Saves, ERA,RA,EA are highly correleated with target variable.\n",
    "    - here is visible multi colinearity between the feature columns \"Earned Runs\", \"Earned Run Average\" and \"Runs Average\". This need to check.\n",
    "\n",
    "<b>This multicollinearity need to check it with varaiance inflation factor or need to address by use of PCA.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Checking Multicollinearity between features using variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif= pd.DataFrame()\n",
    "vif['VIF']= [variance_inflation_factor(df2.values,i) for i in range(df1.shape[1])]\n",
    "vif['Features']= df1.columns\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Comment :\n",
    "<b> It is natural to have such high multicollinearity due to following reason :- </b>\n",
    "    1. Earned Run Average,Earned Runs,Runs Average are highly correlated with each other.\n",
    "    2. At Bat and Hits are 0.771 correlated with each other. But at same time very poor correlated with target variable.\n",
    "    3. <b> Another most important reason is data need to scale which we did not scale until now. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy to Address Multicollinearity :\n",
    "1. Removing Some of highly correlated features. But this will not work here as most of input features are correlated with each other either moderated or poorly.\n",
    "2. Another way to address Multicollinerity is to Scaled Data and then apply PCA.\n",
    "\n",
    "<b> We will go by Second way for further investigation. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df1.drop(columns =['Wins'])\n",
    "Y=df1['Wins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler= StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "#plot the graph to find the principal components\n",
    "x_pca = pca.fit_transform(X_scale)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), 'ro-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance %')\n",
    "plt.title('Explained variance Ratio')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment -\n",
    "<b> AS per the graph, we can see that 7 principal components attribute for 95% of variation in the data.  We shall pick the first 7 components for our prediction </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_new = PCA(n_components=7)\n",
    "x_new = pca_new.fit_transform(X_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principle_x=pd.DataFrame(x_new,columns=np.arange(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Multicollinearity after applying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif= pd.DataFrame()\n",
    "vif['VIF']= [variance_inflation_factor(principle_x.values,i) for i in range(principle_x.shape[1])]\n",
    "vif['Features']= principle_x.columns\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> We can see that  Multicollinearity issue is clear now.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import  GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import  Ridge\n",
    "from sklearn.linear_model import  Lasso\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(principle_x, Y, random_state=42, test_size=.3)\n",
    "print('Training feature matrix size:',X_train.shape)\n",
    "print('Training target vector size:',Y_train.shape)\n",
    "print('Test feature matrix size:',X_test.shape)\n",
    "print('Test target vector size:',Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Finding Best Random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "maxR2_score=0\n",
    "maxRS=0\n",
    "for i in range(1,250):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(principle_x, Y, random_state=i, test_size=.25)\n",
    "    lin_reg=LinearRegression()\n",
    "    lin_reg.fit(X_train,Y_train)\n",
    "    y_pred=lin_reg.predict(X_test)\n",
    "    R2=r2_score(Y_test,y_pred)\n",
    "    if R2>maxR2_score:\n",
    "        maxR2_score=R2\n",
    "        maxRS=i\n",
    "print('Best R2 Score is', maxR2_score ,'on Random_state', maxRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression : Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(principle_x, Y, random_state=217, test_size=.25)\n",
    "lin_reg=LinearRegression()\n",
    "lin_reg.fit(X_train,Y_train)\n",
    "lin_reg.score(X_train,Y_train)\n",
    "y_pred=lin_reg.predict(X_test)\n",
    "print('\\033[1m'+'Predicted Wins:'+'\\033[0m\\n',y_pred)\n",
    "print('\\n')\n",
    "print('\\033[1m'+'Actual Wins:'+'\\033[0m\\n',Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Evaluation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[1m'+' Error :'+'\\033[0m')\n",
    "print('Mean absolute error :', mean_absolute_error(Y_test,y_pred))\n",
    "print('Mean squared error :', mean_squared_error(Y_test,y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(mean_squared_error(Y_test,y_pred)))\n",
    "print('\\n')\n",
    "from sklearn.metrics import r2_score\n",
    "print('\\033[1m'+' R2 Score :'+'\\033[0m')\n",
    "print(r2_score(Y_test,y_pred,multioutput='variance_weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "y_pred=lin_reg.predict(X_test)\n",
    "sns.swarmplot(Y_test.round(2), y_pred)\n",
    "print('\\033[1m'+' True Values Vs Predicted Value plot :' +'\\033[0m')\n",
    "plt.xlabel('True Values' , fontsize=15)\n",
    "plt.ylabel('Predictions', fontsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding best n_neighbors for KNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import neighbors\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse_val = [] #to store rmse values for different k\n",
    "for K in range(10):\n",
    "    K = K+1\n",
    "    model = neighbors.KNeighborsRegressor(n_neighbors = K)\n",
    "\n",
    "    model.fit(X_train,Y_train)  #fit the model\n",
    "    y_pred=model.predict(X_test) #make prediction on test set\n",
    "    error = sqrt(mean_squared_error(Y_test,y_pred)) #calculate rmse\n",
    "    rmse_val.append(error) #store rmse values\n",
    "    print('RMSE value for k= ' , K , 'is:', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the rmse values against k values -\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(range(10), rmse_val, color='blue', linestyle='dashed', marker='o', markerfacecolor='green', markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment- \n",
    "At k= 4, we get the minimum RMSE value which approximately 5.050525962709231, and shoots up on further increasing the k value. We can safely say that k=4 will give us the best result in this case\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying other Regression Model, Evaluation & Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestRegressor(n_estimators = 250 ,max_depth=6)\n",
    "# svr=SVR(C=1.0, epsilon=0.2, kernel='poly', gamma='auto')\n",
    "# dtc = DecisionTreeRegressor(criterion='mse')\n",
    "# adb=AdaBoostRegressor(learning_rate=0.1)\n",
    "# gradb=GradientBoostingRegressor( max_depth=6,learning_rate=0.1)\n",
    "# knn=KNeighborsRegressor(n_neighbors=4,algorithm='kd_tree')\n",
    "# ls= Lasso(alpha=1e-2, normalize=True, max_iter=1e5)\n",
    "# rd=Ridge(alpha=1e-2, normalize=True)\n",
    "# xgb=XGBRegressor()\n",
    "# model = [rf,ls,rd,svr,dtc,adb,gradb,knn,xgb]\n",
    "\n",
    "# for m in model:\n",
    "#     m.fit(X_train,Y_train)\n",
    "#     m.score(X_train,Y_train)\n",
    "#     y_pred = m.predict(X_test)\n",
    "#     print('\\n')                                        \n",
    "#     print('\\033[1m'+' Error of ', m, ':' +'\\033[0m')\n",
    "#     print('Mean absolute error :', mean_absolute_error(Y_test,y_pred))\n",
    "#     print('Mean squared error :', mean_squared_error(Y_test,y_pred))\n",
    "#     print('Root Mean Squared Error:', np.sqrt(mean_squared_error(Y_test,y_pred)))\n",
    "#     print('\\n')\n",
    "\n",
    "#     print('\\033[1m'+' R2 Score :'+'\\033[0m')\n",
    "#     print(r2_score(Y_test,y_pred)) \n",
    "    \n",
    "#     # Cross Validation\n",
    "# #     score = cross_val_score(m, principle_x, Y, cv =4)\n",
    "#     print('\\n')\n",
    "# #     print('\\033[1m'+'Cross Validation Score :',m,\":\"+'\\033[0m\\n')\n",
    "#     print(\"Mean CV Score :\",score.mean())\n",
    "#     print('==============================================================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we can see that Lasso Regression gives maximum R2 score of 91.90%. So we will apply Hyperparameter tuning on Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = [rf,ls,rd,svr,dtc,adb,gradb,knn,xgb]\n",
    "\n",
    "# for m in model:\n",
    "#     plt.figure(figsize=(7,5))\n",
    "#     m.fit(X_train,Y_train)\n",
    "#     y_pred=m.predict(X_test)\n",
    "#     print('\\n')\n",
    "#     print('\\033[1m'+' True Values Vs Predicted Value plot', m, ':' +'\\033[0m')\n",
    "#     sns.scatterplot(Y_test.round(2), y_pred)\n",
    "#     plt.xlabel('True Values' , fontsize=15)\n",
    "#     plt.ylabel('Predictions', fontsize=15)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "#     print('\\n')\n",
    "#     print('===================================================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning : GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = {'alpha':np.array([1,0.5,0.1,0.01,0.001,0.0001]),\n",
    "             'fit_intercept': [True,False],'normalize':[True,False],\n",
    "             'max_iter':[250,500,1000,1500],'random_state':np.arange(100),\n",
    "             'selection':[\"cyclic\",\"random\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCV = GridSearchCV(Lasso(),parameter,cv=5,n_jobs = -1,verbose = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCV.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_mod = Lasso(alpha = 0.5, fit_intercept= True, normalize = False,\n",
    "                                           max_iter = 250, random_state = 32,selection ='random')\n",
    "Final_mod.fit(X_train,Y_train)\n",
    "y_pred=Final_mod.predict(X_test)\n",
    "print('\\n')                                        \n",
    "print('\\033[1m'+' Error in Final Model :' +'\\033[0m')\n",
    "print('Mean absolute error :', mean_absolute_error(Y_test,y_pred))\n",
    "print('Mean squared error :', mean_squared_error(Y_test,y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(mean_squared_error(Y_test,y_pred)))\n",
    "print('\\n')\n",
    "print('\\033[1m'+' R2 Score of Final Model :'+'\\033[0m')\n",
    "print(r2_score(Y_test,y_pred)) \n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "y_pred=Final_mod.predict(X_test)\n",
    "sns.swarmplot(Y_test.round(2), y_pred)\n",
    "print('\\033[1m'+' True Values Vs Predicted Value plot :' +'\\033[0m')\n",
    "plt.xlabel('True Values' , fontsize=15)\n",
    "plt.ylabel('Predictions', fontsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(Final_mod,'Baseballn_Final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
